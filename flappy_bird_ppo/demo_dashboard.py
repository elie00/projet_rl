#!/usr/bin/env python3
"""
Demo Dashboard Generator - Creates sample dashboard with synthetic data
"""
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from datetime import datetime
from metrics_dashboard import MetricsDashboard

# Set style for professional plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def generate_synthetic_training_data(num_episodes=500):
    """Generate realistic synthetic training data for demo"""
    
    training_history = []
    
    # Simulate training progression
    base_reward = -10
    improvement_rate = 0.02
    noise_level = 5
    
    for episode in range(num_episodes):
        # Simulate improving performance with some noise
        expected_reward = base_reward + episode * improvement_rate
        actual_reward = expected_reward + np.random.normal(0, noise_level)
        
        # Simulate decreasing losses
        actor_loss = 0.5 * np.exp(-episode/200) + np.random.normal(0, 0.1)
        critic_loss = 1.0 * np.exp(-episode/150) + np.random.normal(0, 0.1)
        
        # Simulate entropy decrease (exploration to exploitation)
        entropy = 0.8 * np.exp(-episode/300) + 0.1 + np.random.normal(0, 0.05)
        
        # Simulate KL divergence
        kl_div = 0.05 + np.random.exponential(0.02)
        
        # Create episode rewards (multiple per episode)
        episode_rewards = []
        for _ in range(np.random.randint(1, 4)):  # 1-3 episodes per training step
            episode_rewards.append(actual_reward + np.random.normal(0, 2))
        
        # Create synthetic metrics entry
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'episode': episode,
            'episode_rewards': episode_rewards,
            'training': {
                'actor_loss': max(0.001, actor_loss),
                'critic_loss': max(0.001, critic_loss),
                'entropy': max(0.01, entropy),
                'kl_divergence': max(0.001, kl_div),
                'returns_mean': actual_reward,
                'returns_std': noise_level,
            },
            'performance_metrics': {
                'reward_trend_slope': improvement_rate + np.random.normal(0, 0.005),
                'reward_stability': max(0.1, noise_level - episode/100),
                'learning_efficiency': min(1.0, episode/300),
                'convergence_indicator': min(1.0, episode/400)
            }
        }
        
        # Add evaluation stats every 50 episodes
        if episode % 50 == 0 and episode > 0:
            eval_reward = actual_reward + np.random.normal(0, 1)
            metrics['evaluation'] = {
                'eval_reward_mean': eval_reward,
                'eval_reward_std': 2.0,
                'eval_reward_min': eval_reward - 5,
                'eval_reward_max': eval_reward + 3,
                'eval_length_mean': 50 + episode/10,
                'eval_length_std': 10,
                'success_rate': min(1.0, max(0.0, (episode - 100)/400)),
                'num_episodes': 10
            }
        
        training_history.append(metrics)
    
    return training_history

def create_demo_dashboard():
    """Create demonstration dashboard"""
    print("üöÄ G√©n√©ration du dashboard de d√©monstration...")
    
    # Create directories
    os.makedirs("metrics", exist_ok=True)
    os.makedirs("dashboard_exports", exist_ok=True)
    
    # Generate synthetic data
    print("üìä G√©n√©ration de donn√©es d'entra√Ænement synth√©tiques...")
    training_history = generate_synthetic_training_data(500)
    
    # Save synthetic metrics
    for i, metrics in enumerate(training_history[::10]):  # Save every 10th episode
        filename = f"metrics/metrics_episode_{metrics['episode']}.json"
        with open(filename, 'w') as f:
            json.dump(metrics, f, indent=2)
    
    # Create dashboard
    dashboard = MetricsDashboard()
    dashboard.training_history = training_history
    
    print("üé® G√©n√©ration du dashboard principal...")
    dashboard_path = dashboard.create_comprehensive_dashboard(save_individual=True)
    
    print("üìà Export du r√©sum√© des m√©triques...")
    summary_path = dashboard.export_metrics_summary()
    
    # Generate presentation slides
    print("üéØ G√©n√©ration des slides de pr√©sentation...")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Slide 1: Training Overview
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('PPO Flappy Bird - Aper√ßu de l\'Entra√Ænement', fontsize=20, fontweight='bold')
    
    episodes = [m['episode'] for m in training_history]
    dashboard._plot_episode_rewards(ax1, episodes)
    dashboard._plot_training_losses(ax2, episodes)
    dashboard._plot_performance_metrics(ax3, episodes)
    dashboard._plot_policy_evolution(ax4, episodes)
    
    plt.tight_layout()
    slide1_path = f'dashboard_exports/slide_1_apercu_{timestamp}.png'
    plt.savefig(slide1_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Slide 2: Performance Analysis
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('PPO Flappy Bird - Analyse de Performance', fontsize=20, fontweight='bold')
    
    dashboard._plot_learning_efficiency(ax1, episodes)
    dashboard._plot_convergence_analysis(ax2, episodes)
    dashboard._plot_statistical_summary(ax3)
    dashboard._plot_performance_heatmap(ax4)
    
    plt.tight_layout()
    slide2_path = f'dashboard_exports/slide_2_performance_{timestamp}.png'
    plt.savefig(slide2_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Slide 3: Summary with French text
    fig, ax = plt.subplots(figsize=(16, 12))
    fig.suptitle('PPO Flappy Bird - R√©sum√© des M√©triques Cl√©s', fontsize=20, fontweight='bold')
    
    latest = training_history[-1]
    
    summary_text = f"""
R√âSUM√â DE L'ENTRA√éNEMENT PPO FLAPPY BIRD
{'='*60}

CONFIGURATION:
‚Ä¢ Algorithme: Proximal Policy Optimization (PPO)
‚Ä¢ Environnement: Flappy Bird avec capteurs LiDAR
‚Ä¢ √âpisodes d'entra√Ænement: {len(training_history)}
‚Ä¢ √âpisode final: {latest.get('episode', 'N/A')}

M√âTRIQUES DE PERFORMANCE FINALES:
‚Ä¢ Loss Acteur: {latest.get('training', {}).get('actor_loss', 0):.4f}
‚Ä¢ Loss Critique: {latest.get('training', {}).get('critic_loss', 0):.4f}
‚Ä¢ Entropie Politique: {latest.get('training', {}).get('entropy', 0):.4f}
‚Ä¢ KL Divergence: {latest.get('training', {}).get('kl_divergence', 0):.4f}

ANALYSE D'APPRENTISSAGE:
‚Ä¢ Pente de Tendance des R√©compenses: {latest.get('performance_metrics', {}).get('reward_trend_slope', 0):.4f}
‚Ä¢ Efficacit√© d'Apprentissage: {latest.get('performance_metrics', {}).get('learning_efficiency', 0):.4f}
‚Ä¢ Indicateur de Convergence: {latest.get('performance_metrics', {}).get('convergence_indicator', 0):.4f}
‚Ä¢ Stabilit√© des R√©compenses: {latest.get('performance_metrics', {}).get('reward_stability', 0):.4f}

R√âSULTATS D'√âVALUATION:
‚Ä¢ R√©compense Moyenne: {latest.get('evaluation', {}).get('eval_reward_mean', 0):.2f}
‚Ä¢ R√©compense Maximale: {latest.get('evaluation', {}).get('eval_reward_max', 0):.2f}
‚Ä¢ Taux de Succ√®s: {latest.get('evaluation', {}).get('success_rate', 0):.1%}
‚Ä¢ Longueur Moyenne d'√âpisode: {latest.get('evaluation', {}).get('eval_length_mean', 0):.1f}

CONCLUSIONS:
‚úì Am√©lioration constante des performances observ√©e
‚úì Convergence stable de la politique atteinte
‚úì √âquilibre exploration-exploitation efficace
‚úì Agent pr√™t pour d√©ploiement ou optimisation avanc√©e

RECOMMANDATIONS POUR LA PR√âSENTATION:
‚Ä¢ Utiliser le graphique de r√©compenses pour montrer la progression
‚Ä¢ Mettre en avant la diminution des losses (stabilit√© d'entra√Ænement)
‚Ä¢ Souligner le taux de succ√®s et les m√©triques d'efficacit√©
‚Ä¢ Comparer avec une baseline al√©atoire si disponible
    """
    
    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, 
           fontsize=11, verticalalignment='top', fontfamily='monospace',
           bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis('off')
    
    slide3_path = f'dashboard_exports/slide_3_resume_{timestamp}.png'
    plt.savefig(slide3_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return dashboard_path, summary_path, [slide1_path, slide2_path, slide3_path]

if __name__ == "__main__":
    dashboard_path, summary_path, slide_paths = create_demo_dashboard()
    
    print("\nüéâ DASHBOARD G√âN√âR√â AVEC SUCC√àS!")
    print("=" * 50)
    print(f"üìä Dashboard principal: {dashboard_path}")
    print(f"üìã R√©sum√© des m√©triques: {summary_path}")
    print("\nüéØ Slides de pr√©sentation g√©n√©r√©es:")
    for i, slide_path in enumerate(slide_paths, 1):
        print(f"   Slide {i}: {slide_path}")
    
    print(f"\nüìÅ Tous les fichiers sont dans: dashboard_exports/")
    print("‚ú® Pr√™t pour vos pr√©sentations!")