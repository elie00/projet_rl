# üöÄ Guide de Prise en Main - Flappy Bird PPO

Ce guide vous accompagne pas √† pas pour comprendre, installer et utiliser ce projet d'apprentissage par renforcement avec l'algorithme PPO.

## üìã Table des Mati√®res

1. [Vue d'ensemble du projet](#vue-densemble-du-projet)
2. [Pr√©requis et installation](#pr√©requis-et-installation)
3. [Premi√®re utilisation](#premi√®re-utilisation)
4. [Comprendre la structure](#comprendre-la-structure)
5. [Guide des commandes](#guide-des-commandes)
6. [Interpr√©ter les r√©sultats](#interpr√©ter-les-r√©sultats)
7. [Personnaliser l'entra√Ænement](#personnaliser-lentra√Ænement)
8. [R√©solution de probl√®mes](#r√©solution-de-probl√®mes)
9. [Cas d'usage avanc√©s](#cas-dusage-avanc√©s)

## üéØ Vue d'ensemble du projet

### Qu'est-ce que ce projet fait ?

Ce projet entra√Æne un agent intelligent √† jouer au jeu Flappy Bird en utilisant l'**apprentissage par renforcement**. L'agent apprend par essais et erreurs, sans instructions pr√©alables, jusqu'√† ma√Ætriser parfaitement le jeu.

### Pourquoi utiliser PPO ?

**PPO (Proximal Policy Optimization)** est un algorithme de pointe qui :
- ‚úÖ Apprend efficacement avec peu d'√©chantillons
- ‚úÖ Reste stable pendant l'entra√Ænement
- ‚úÖ Fonctionne bien sur de nombreux environnements
- ‚úÖ Est utilis√© dans l'industrie (OpenAI, DeepMind)

### R√©sultats attendus

Apr√®s entra√Ænement, votre agent devrait :
- Obtenir **30-80 pipes en moyenne** (vs -7 pour une politique al√©atoire)
- Atteindre un **taux de succ√®s de 100%**
- Naviguer avec **fluidit√© et pr√©cision** dans le jeu

## üíª Pr√©requis et installation

### Syst√®me requis

- **Python 3.8+** (recommand√© : Python 3.10)
- **4 GB RAM minimum** (8 GB recommand√©)
- **2 GB d'espace disque**
- **MacOS, Linux ou Windows**

### Installation √©tape par √©tape

1. **V√©rifiez votre version Python :**
   ```bash
   python3.10 --version
   # Devrait afficher : Python 3.10.x
   ```

2. **Naviguez dans le r√©pertoire du projet :**
   ```bash
   cd flappy_bird_ppo
   ```

3. **Installez les d√©pendances :**
   ```bash
   # Installation des d√©pendances de base
   pip3.10 install -r requirements.txt
   
   # Installation de TensorFlow compatible
   pip3.10 install tensorflow==2.16.1
   
   # Installation des outils vid√©o
   pip3.10 install "gymnasium[other]" moviepy
   ```

4. **Testez l'installation :**
   ```bash
   python3.10 -c "import torch, gymnasium, flappy_bird_gymnasium; print('‚úÖ Installation r√©ussie !')"
   ```

### R√©solution d'erreurs d'installation

**Erreur : "command not found: python3.10"**
```bash
# Sur macOS avec Homebrew
brew install python@3.10

# Sur Ubuntu/Debian
sudo apt install python3.10 python3.10-pip

# Sur Windows, t√©l√©chargez depuis python.org
```

**Erreur : "No module named 'torch'"**
```bash
pip3.10 install --upgrade torch torchvision
```

## üèÉ‚Äç‚ôÇÔ∏è Premi√®re utilisation

### Test rapide (5 minutes)

1. **Lancement d'un entra√Ænement court :**
   ```bash
   python3.10 main.py --mode train
   ```
   Vous devriez voir :
   ```
   Starting PPO training for Flappy Bird
   Device: cpu
   Total episodes: 10000
   Environment: FlappyBird-v0
   ```

2. **Laissez tourner 2-3 minutes**, puis arr√™tez avec `Ctrl+C`

3. **√âvaluez l'agent :**
   ```bash
   python3.10 main.py --mode eval --model_path models/best_model.pt --episodes 5
   ```

### Que se passe-t-il pendant l'entra√Ænement ?

1. **Collecte d'exp√©rience** : L'agent joue 512 √©tapes
2. **Calcul des r√©compenses** : √âvalue les bonnes/mauvaises actions
3. **Mise √† jour des r√©seaux** : Am√©liore la politique de jeu
4. **√âvaluation p√©riodique** : Teste les performances tous les 100 √©pisodes
5. **Sauvegarde automatique** : Garde le meilleur mod√®le

## üìÅ Comprendre la structure

```
flappy_bird_ppo/
‚îú‚îÄ‚îÄ üìÑ main.py              # Point d'entr√©e principal
‚îú‚îÄ‚îÄ ‚öôÔ∏è config.py            # Tous les param√®tres
‚îú‚îÄ‚îÄ üß† models.py            # R√©seaux Actor et Critic
‚îú‚îÄ‚îÄ ü§ñ ppo_agent.py         # Logique d'apprentissage PPO
‚îú‚îÄ‚îÄ üõ†Ô∏è utils.py             # Fonctions utilitaires
‚îú‚îÄ‚îÄ üìã requirements.txt     # Liste des d√©pendances
‚îú‚îÄ‚îÄ üìñ README.md            # Documentation compl√®te
‚îú‚îÄ‚îÄ üìñ GUIDE_PRISE_EN_MAIN.md # Ce guide
‚îú‚îÄ‚îÄ üìÅ videos/              # Vid√©os d'√©valuation
‚îú‚îÄ‚îÄ üìÅ logs/                # Logs TensorBoard
‚îî‚îÄ‚îÄ üìÅ models/              # Mod√®les sauvegard√©s
```

### Fichiers cl√©s √† conna√Ætre

**`config.py`** - Tous les param√®tres :
- Hyperparam√®tres PPO
- Param√®tres d'entra√Ænement
- Configuration du logging

**`main.py`** - Interface principale :
- Mode entra√Ænement
- Mode √©valuation
- Mode visualisation

**`models/best_model.pt`** - Meilleur mod√®le sauvegard√©

## üéÆ Guide des commandes

### Commandes essentielles

**1. Entra√Ænement complet :**
```bash
python3.10 main.py --mode train
```
- Dur√©e : 2-4 heures
- Cr√©e : logs, vid√©os, mod√®les
- Sortie : Rapport final avec statistiques

**2. √âvaluation d'un mod√®le :**
```bash
python3.10 main.py --mode eval --model_path models/best_model.pt --episodes 10
```
- Tests le mod√®le sur 10 √©pisodes
- Affiche statistiques d√©taill√©es
- Compare avec baseline al√©atoire

**3. Regarder l'agent jouer :**
```bash
python3.10 main.py --mode watch --model_path models/best_model.pt --episodes 3
```
- Ouvre une fen√™tre de jeu
- Montre l'agent en action
- Parfait pour les d√©monstrations

### Commandes avanc√©es

**Reprendre un entra√Ænement :**
```bash
python3.10 main.py --mode train --resume models/ppo_flappy_bird_1000.pt
```

**√âvaluation avec rendu :**
```bash
python3.10 main.py --mode eval --model_path models/best_model.pt --episodes 20 --render
```

**Test rapide :**
```bash
python3.10 main.py --mode eval --model_path models/best_model.pt --episodes 3
```

## üìä Interpr√©ter les r√©sultats

### Pendant l'entra√Ænement

**Sortie console typique :**
```
Episode 500/10000
  Avg Reward (last 100): 13.57
  Max Reward (last 100): 27.20
  Actor Loss: -0.0012
  Critic Loss: 1.5072
  Time per episode: 0.16s
  Total steps: 256512
```

**Comment lire ces m√©triques :**
- **Avg Reward** : Score moyen r√©cent (plus c'est haut, mieux c'est)
- **Max Reward** : Meilleur score r√©cent
- **Actor Loss** : Perte de la politique (devrait diminuer)
- **Critic Loss** : Perte de l'√©valuation d'√©tat
- **Time per episode** : Vitesse d'entra√Ænement

### Apr√®s √©valuation

**Rapport d'√©valuation :**
```
TRAINED AGENT PERFORMANCE:
  Average Reward: 35.10 ¬± 20.32
  Best Reward: 66.90
  Success Rate: 100.0%

PERFORMANCE COMPARISON:
  Reward Improvement: +42.55 (+571.4%)
  ‚úì Agent outperforms random baseline!
```

**Crit√®res de succ√®s :**
- **Reward moyen > 10** : Agent comp√©tent
- **Success Rate > 80%** : Performance consistante
- **Am√©lioration > 300%** : Apprentissage significatif

### Visualiser avec TensorBoard

```bash
tensorboard --logdir logs/
```
Puis ouvrez http://localhost:6006 pour voir :
- Courbes d'apprentissage
- √âvolution des losses
- Statistiques d'entra√Ænement

## ‚öôÔ∏è Personnaliser l'entra√Ænement

### Modifier les hyperparam√®tres

√âditez `config.py` :

```python
# Pour un entra√Ænement plus rapide (moins pr√©cis)
MAX_STEPS_PER_ROLLOUT = 256    # Au lieu de 512
PPO_EPOCHS = 2                 # Au lieu de 4

# Pour plus d'exploration
ENTROPY_COEFF = 0.05           # Au lieu de 0.01

# Pour un entra√Ænement plus long
TOTAL_EPISODES = 20000         # Au lieu de 10000
```

### Configurations pr√©d√©finies

**Configuration "Rapide" (test) :**
```python
TOTAL_EPISODES = 1000
MAX_STEPS_PER_ROLLOUT = 256
EVAL_FREQUENCY = 50
```

**Configuration "Performance" (production) :**
```python
TOTAL_EPISODES = 20000
MAX_STEPS_PER_ROLLOUT = 1024
PPO_EPOCHS = 8
```

**Configuration "Demo" (pr√©sentation) :**
```python
RECORD_VIDEO = True
EVAL_FREQUENCY = 50
RENDER_EVAL = False
```

### D√©sactiver fonctionnalit√©s

**Pas de vid√©os (plus rapide) :**
```python
RECORD_VIDEO = False
```

**Pas de Weights & Biases :**
```python
USE_WANDB = False
```

## üîß R√©solution de probl√®mes

### Erreurs courantes

**1. "ModuleNotFoundError: No module named 'torch'"**
```bash
pip3.10 install torch torchvision
```

**2. "CUDA out of memory"**
Dans `config.py` :
```python
DEVICE = "cpu"  # Force l'utilisation du CPU
MAX_STEPS_PER_ROLLOUT = 256  # R√©duit la m√©moire
```

**3. "Render mode is None"**
Erreur corrig√©e automatiquement dans le code.

**4. L'agent ne s'am√©liore pas**
- V√©rifiez que `LEARNING_RATE = 3e-4` (pas trop haut/bas)
- Augmentez `TOTAL_EPISODES`
- V√©rifiez les rewards dans les logs

### Performance lente

**Sur CPU :**
- R√©duisez `MAX_STEPS_PER_ROLLOUT` √† 256
- D√©sactivez l'enregistrement vid√©o
- Utilisez moins d'√©pisodes pour les tests

**Manque d'espace disque :**
- Vid√©os prennent ~500 MB pour un entra√Ænement complet
- Supprimez les anciennes vid√©os dans `videos/`
- D√©sactivez `RECORD_VIDEO = False`

### Debugging avanc√©

**Mode verbose :**
```python
# Dans main.py, ajoutez au d√©but :
import logging
logging.basicConfig(level=logging.DEBUG)
```

**V√©rifier les gradients :**
```python
# Dans config.py :
MAX_GRAD_NORM = 0.5  # Clipping plus strict
```

## üéØ Cas d'usage avanc√©s

### 1. Comparaison d'hyperparam√®tres

Cr√©ez plusieurs configs et comparez :

```bash
# Config 1 : LR √©lev√©
python3.10 main.py --mode train  # Modifiez config.py avec LR=1e-3

# Config 2 : LR faible  
python3.10 main.py --mode train  # Modifiez config.py avec LR=1e-5

# Comparez les r√©sultats dans logs/
```

### 2. Transfert d'apprentissage

```bash
# Entra√Ænez sur un environnement
python3.10 main.py --mode train

# Reprenez sur un environnement modifi√©
python3.10 main.py --mode train --resume models/best_model.pt
```

### 3. Ensemble de mod√®les

Entra√Ænez plusieurs mod√®les avec des graines diff√©rentes :
```python
# config.py
SEED = 42  # Premier mod√®le
SEED = 123 # Deuxi√®me mod√®le  
SEED = 456 # Troisi√®me mod√®le
```

### 4. Analyse de robustesse

Testez sur diff√©rents environnements :
```bash
# Testez sur plusieurs √©pisodes
python3.10 main.py --mode eval --model_path models/best_model.pt --episodes 100

# Analysez les statistiques de variance
```

## üéì Conseils p√©dagogiques

### Pour un cours/pr√©sentation

1. **D√©monstration baseline** (2 min) :
   - Montrez une politique al√©atoire qui √©choue
   - Score typique : -7 √† 0 pipes

2. **Lancement entra√Ænement** (5 min) :
   - Expliquez PPO rapidement
   - Montrez les m√©triques en temps r√©el

3. **Agent entra√Æn√©** (3 min) :
   - D√©monstration avec `--mode watch`
   - Scores typiques : 30-80 pipes

### Pour un projet √©tudiant

**Exp√©rimentations possibles :**
- Comparer PPO vs politique al√©atoire
- Impact des hyperparam√®tres
- Courbes d'apprentissage
- Analyse de convergence

**Livrables sugg√©r√©s :**
- Code fonctionnel ‚úÖ
- Vid√©os d'√©valuation üìπ
- Rapport avec m√©triques üìä
- Analyse des r√©sultats üìà

## üìö Ressources suppl√©mentaires

### Documentation technique
- [Article PPO original](https://arxiv.org/abs/1707.06347)
- [Gymnasium documentation](https://gymnasium.farama.org/)
- [PyTorch RL tutorials](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

### Environnements similaires
- CartPole-v1 : Plus simple, pour d√©buter
- Atari Breakout : Plus complexe, plus long
- Lunar Lander : Probl√®me de contr√¥le continu

### Extensions possibles
- **Autres algorithmes** : A3C, SAC, TD3
- **Autres jeux** : Super Mario Bros, Pac-Man
- **R√©seaux avanc√©s** : CNN pour images, LSTM pour m√©moire

---

## üéâ F√©licitations !

Si vous √™tes arriv√© jusqu'ici, vous ma√Ætrisez maintenant :
- ‚úÖ L'installation et utilisation du projet
- ‚úÖ L'interpr√©tation des r√©sultats
- ‚úÖ La personnalisation des param√®tres
- ‚úÖ Le d√©bogage des probl√®mes courants

Votre agent PPO devrait obtenir des scores impressionnants (30-80 pipes) et vous avez toutes les cl√©s pour impressionner votre audience ! üöÄ

**Bon apprentissage par renforcement !** ü§ñ